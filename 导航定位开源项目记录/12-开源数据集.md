<div align="center">
<h1>导航定位建图开源数据集</h1>
</div>

<div align="center">
    <img alt="Static Badge" src="https://img.shields.io/badge/QQ-1482275402-red">
    <img alt="Static Badge" src="https://img.shields.io/badge/%E5%BE%AE%E4%BF%A1-lizhengxiao99-green">
    <img alt="Static Badge" src="https://img.shields.io/badge/Email-dauger%40126.com-brown">
    <a href="https://blog.csdn.net/daoge2666/"><img src="https://img.shields.io/badge/CSDN-论坛-c32136" /></a>
    <a href="https://www.zhihu.com/people/dao-ge-92-60/"><img src="https://img.shields.io/badge/Zhihu-知乎-blue" /></a>
    <img src="https://komarev.com/ghpvc/?username=LiZhengXiao99&label=Views&color=0e75b6&style=flat" alt="访问量统计" />
</div>
<br/>

> 说明：
>
> * GNSS 
> * 还有哪里有数据？①科研团队的Github或者官网、②写的完善开源程序都会提供测试数据、③论文附带、④设备厂商提供测试数据、⑤综述论文介绍。
> * 使用开源数据了，发论文的时候应当说明来源，引用对应的参考文献，向对数据提供者致谢。
> * 为了防止数据下载链接失效、作者删除、网站进不去了等情况，建议将重要的数据备份。

---

| [PSINS 网站](http://www.psins.org.cn/dhsj)、[i2NAV-awesome-gins-datasets](https://github.com/i2Nav-WHU/awesome-gins-datasets)、 [weisongwen-UrbanNavDataset](https://github.com/weisongwen/UrbanNavDataset)、[UrbanLoco](https://github.com/weisongwen/UrbanLoco)、[PPPLib-Dataset](https://github.com/heiwa0519/PPPLib-Dataset)、[gici-open-dataset](https://github.com/chichengcn/gici-open-dataset)、[GVINS-Dataset](https://github.com/HKUST-Aerial-Robotics/GVINS-Dataset)、[gnsstk-data](https://github.com/SGL-UT/gnsstk-data)、[GNSS_RTK_data](https://github.com/jdesbonnet/GNSS_RTK_data)、[cssrlib-data](https://github.com/hirokawa/cssrlib-data)、[comma2k19](https://github.com/commaai/comma2k19)、[SJTU_GVI](https://github.com/sjtuyinjie/SJTU_GVI)、[Railway-Precise-Localization](https://github.com/ETH-PBL/Railway-Precise-Localization)、[precision-gnss](https://github.com/amazon-science/precision-gnss)、[SYSU-Campus-GVI-Dataset](https://github.com/SYSU-CPNTLab/SYSU-Campus-GVI-Dataset)、[GVI-SYSU-Outdoor-Indoor-Dataset](https://github.com/SYSU-CPNTLab/GVI-SYSU-Outdoor-Indoor-Dataset)、[S3E](https://github.com/PengYu-Team/S3E)、[LBL-AQUALOC-Dataset](https://github.com/SYSU-CPNTLab/LBL-AQUALOC-Dataset)、[nebula-odometry-dataset](https://github.com/NeBula-Autonomy/nebula-odometry-dataset)、[vio-gnss-dataset](https://github.com/AaltoVision/vio-gnss-dataset)、[cssrlib-data](https://github.com/iGNSS/cssrlib-data)、[comma2k19](https://github.com/commaai/comma2k19)、[IMU_dataset](https://github.com/miguelrasteiro/IMU_dataset)、[HR_IMU_falldetection_dataset](https://github.com/nhoyh/HR_IMU_falldetection_dataset)、[imu_comparison_data](https://github.com/eruffaldi/imu_comparison_data)、[HumanInertialPose](https://github.com/ManuelPalermo/HumanInertialPose)、[H2LID](https://github.com/duanxz0127/H2LID)、[FusionPoser](https://github.com/LuzyCat/FusionPoser)、[ECC_dataset](https://github.com/AtDinesh/ECC_dataset)、[TRIPOD](https://github.com/HPI-CH/TRIPOD)、[broad](https://github.com/dlaidig/broad)、[BROAD_IMU_dataset](https://github.com/Armanasq/BROAD_IMU_dataset)、[HAR_IMU_Stretch](https://github.com/thunguyenth/HAR_IMU_Stretch)、[Genshin-Impact-Dataset](https://github.com/zhaoxuhui/Genshin-Impact-Dataset)、[utbm_robocar_dataset](https://github.com/epan-utbm/utbm_robocar_dataset)、[uwb-drone-dataset](https://github.com/TIERS/uwb-drone-dataset)、[UWB_TDOA_dataset](https://github.com/Williamwenda/UWB_TDOA_dataset)、[Industrial-UWB-localization-CIR-dataset](https://github.com/JaronFontaine/Industrial-UWB-localization-CIR-dataset)、[M2DGR](https://github.com/SJTU-ViSYS/M2DGR)、[Roller-Coaster-SLAM-Dataset](https://github.com/Factor-Robotics/Roller-Coaster-SLAM-Dataset)、[IR-UWB-Radar-Signal-Dataset-for-Dense-People-Counting](https://github.com/yangxiuzhu777/IR-UWB-Radar-Signal-Dataset-for-Dense-People-Counting)、[UWB_Dataset](https://github.com/unmannedlab/UWB_Dataset)、[uwb-nlos-human-detection](https://github.com/disi-unibo-nlp/uwb-nlos-human-detection)、[snapshot-gnss-data1](https://github.com/JonasBchrt/snapshot-gnss-data)/[2](https://github.com/JonasBchrt/snapshot-gnss-data-2)、[IURHA2023](https://github.com/njursi/IURHA2023)、[WHU-TLS](https://github.com/WHU-USI3DV/WHU-TLS)、[WHU-Helmet](https://github.com/kafeiyin00/WHU-HelmetDataset)、[WHU-Urban-3D](https://whu3d.com/)、[WHU-Railway3D](https://github.com/WHU-USI3DV/WHU-Railway3D)、[WHU-Smartphone-Dataset](https://github.com/CJQP/WHU-Smartphone-Dataset)、[GNSSbuoy](https://github.com/yangleir/GNSSbuoy)、[ShoeMountIMU-Dataset](https://github.com/ECG-XMU/ShoeMountIMU-Dataset)、[legkilo-dataset](https://github.com/ouguangjun/legkilo-dataset)、[SIMUL-dataset](https://github.com/simpleLoc/SIMUL-dataset)、[Meshed_IMU_Garment_HAR_Dataset](https://github.com/eno-lab/Meshed_IMU_Garment_HAR_Dataset)、[PINS-datasets-based-on-Xsens-IMU](https://github.com/RuijieXu0408/PINS-datasets-based-on-Xsens-IMU)、[Millimeter-wave-radar-and-IMU-datasets](https://github.com/jiawendu/Millimeter-wave-radar-and-IMU-datasets)、[Floor-detection-using-IMU-Sensor-Dataset-](https://github.com/Adarshs169/Floor-detection-using-IMU-Sensor-Dataset-)、[drinking-gesture-dataset](https://github.com/Pituohai/drinking-gesture-dataset)、[Synth3DFingerTracking](https://github.com/keving-416/Synth3DFingerTracking)、[IoBT-Smart-Glove](https://github.com/StepUpTransformer/IoBT-Smart-Glove)、[Spinal-Curvature-Dataset](https://github.com/th-alexmak/Spinal-Curvature-Dataset)、[Walking-dataset](https://github.com/Marpino/Walking-dataset)、[Eating-Speed-Dataset](https://github.com/Pituohai/Eating-Speed-Dataset)、[MUESIPCO2020_Dataset](https://github.com/MSBeni/MUESIPCO2020_Dataset)、[Road-Intersection-Re-ID](https://github.com/tjiiv-cprg/Road-Intersection-Re-ID)、[VID-Dataset](https://github.com/ZJU-FAST-Lab/VID-Dataset)、[LIMU-BERT-Public](https://github.com/dapowan/LIMU-BERT-Public)、[ble-rss-fingerprints-calib2021](https://github.com/iGNSS/ble-rss-fingerprints-calib2021)、[humolire](https://github.com/iGNSS/humolire)、[Navigation-Data-Project](https://github.com/ansfl/Navigation-Data-Project)、[DS-Final-Project](https://github.com/jstori/DS-Final-Project)、[cerberus_darpa_subt_datasets](https://github.com/leggedrobotics/cerberus_darpa_subt_datasets)、[GnssData](https://github.com/wvu-navLab/GnssData)、[r3live_dataset](https://github.com/ziv-lin/r3live_dataset)、[BotanicGarden](https://github.com/robot-pesg/BotanicGarden)、[lidar_degeneracy_datasets](https://github.com/ntnu-arl/lidar_degeneracy_datasets)、[Kimera-Multi-Data](https://github.com/MIT-SPARK/Kimera-Multi-Data)、[ntu_viral_dataset](https://github.com/ntu-aris/ntu_viral_dataset)、[treescope](https://github.com/KumarRobotics/treescope)、[Ground-Challenge](https://github.com/sjtuyinjie/Ground-Challenge)、[AirMuseumDataset](https://github.com/AirMuseumDataset/AirMuseumDataset)、[MASSTAR](https://github.com/SYSU-STAR/MASSTAR)、[OpenLane-V2](https://github.com/OpenDriveLab/OpenLane-V2)、[DriveLM](https://github.com/OpenDriveLab/DriveLM)、[LightwheelOcc](https://github.com/OpenDriveLab/LightwheelOcc)、[KITTI数据集](http://www.cvlibs.net/datasets/kitti/eval_object.php)、[GroundTrueHighAccuracyDataset](https://github.com/vauchey/GroundTrueHighAccuracyDataset)[EuRoC数据集](https://projects.asl.ethz.ch/datasets/doku.php?id=kmavvisualinertialdatasets)、[TUM数据集](https://vision.in.tum.de/data/datasets/rgbd-dataset/download)、[Oxford数据集](https://robotcar-dataset.robots.ox.ac.uk/)、[ICL-NUIM数据集](http://www.doc.ic.ac.uk/~ahanda/VaFRIC/iclnuim.html)、[RGB-D对象数据集](http://rgbd-dataset.cs.washington.edu/)、[Dynamic-Scenes 数据集](https://github.com/HaoshengChen/Dynamic-Scenes)、[awesome-slam-datasets 整理](https://github.com/youngguncho/awesome-slam-datasets)、[IMU-datasets 整理](https://github.com/bhumikamittal7/IMU-datasets)、[Awesome_list_of_free_smartphone_GNSS_datasets 整理](https://github.com/mvarga1989/Awesome_list_of_free_smartphone_GNSS_datasets)、 |
| :----------------------------------------------------------: |

---

* 其它开源数据集整理仓库，点开链接 `Ctrl+F` 搜索 `data` 可以快速查找：[awesome-slam-datasets 整理](https://github.com/youngguncho/awesome-slam-datasets)、[IMU-datasets 整理](https://github.com/bhumikamittal7/IMU-datasets)、[barbeau-awesome-gnss 整理](https://github.com/barbeau/awesome-gnss)、[hdkarimi-awesome-gnss 整理](https://github.com/hdkarimi/awesome-gnss)、[mcraymer 整理](https://mcraymer.github.io/geodesy/index.html)、[awesome-visual-slam 整理](https://github.com/tzutalin/awesome-visual-slam)、[Recent_SLAM_Research 整理](https://github.com/YiChenCityU/Recent_SLAM_Research)、[Awesome CV Works 整理](https://vincentqin.tech/posts/awesome-works/)、[Lee-SLAM-source 整理](https://github.com/AlbertSlam/Lee-SLAM-source)、[awesome-slam 整理](https://github.com/kanster/awesome-slam)、[Awesome_Dynamic_SLAM 整理](https://github.com/zhuhu00/Awesome_Dynamic_SLAM)、[awesome-NeRF 整理](https://github.com/awesome-NeRF/awesome-NeRF)、[visual-slam-roadmap 整理](https://github.com/changh95/visual-slam-roadmap)、[Visual_SLAM_Related_Research 整理](https://github.com/wuxiaolang/Visual_SLAM_Related_Research)、[vins-application 整理](https://github.com/engcang/vins-application)、[Recent-Stars-2024 整理](https://github.com/Vincentqyw/Recent-Stars-2024)、[Visual Navigation 整理](https://paperswithcode.com/task/visual-navigation)、[Awesome-Optical-Flow 整理](https://github.com/hzwer/Awesome-Optical-Flow)、[awesome-lidar 整理](https://github.com/szenergy/awesome-lidar)、[awesome-point-cloud-place-recognition 整理](https://github.com/kxhit/awesome-point-cloud-place-recognition)、[awesome-sar 整理](https://github.com/RadarCODE/awesome-sar)、[awesome-radar-perception 整理](https://github.com/ZHOUYI1023/awesome-radar-perception)、[awesome-deep-point-cloud-compression 整理](https://github.com/kaiwangm/awesome-deep-point-cloud-compression)、[awesome-point-cloud-analysis 整理](https://github.com/Yochengliu/awesome-point-cloud-analysis)、[Awesome_Laser_scanners 整理](https://github.com/mvarga1989/Awesome_Laser_scanners)、[Awesome-Human-Activity-Recognition 整理](https://github.com/iGNSS/Awesome-Human-Activity-Recognition)、[Awesome-WiFi-CSI-Research 整理](https://github.com/wuzhiguocarter/Awesome-WiFi-CSI-Research)

* 数据集介绍博客：

  * [GNSS观测数据及各种产品下载网址分享](https://blog.csdn.net/qq_38607471/article/details/129952202)——[GNSS初学者](https://blog.csdn.net/qq_38607471)

  * [GNSS数据下载网址](https://blog.csdn.net/qq_43381998/article/details/132753640)——[Code_ADing](https://blog.csdn.net/qq_43381998)

  * [CDDIS网站下 GNSS 相关的数据产品下载+命名方式解读+文件格式说明文件下载地址](https://blog.csdn.net/Gou_Hailong/article/details/109191352)——[流浪猪头拯救地球](https://blog.csdn.net/Gou_Hailong)

  * [slam数据集整理（持续更新）](https://blog.csdn.net/weixin_41965898/article/details/121771406)——[lucky li](https://blog.csdn.net/weixin_41965898)

  * [【论文阅读】多传感器SLAM数据集](https://blog.csdn.net/weixin_43849505/article/details/136436575)——[Ayakanoinu](https://zhangzhh.blog.csdn.net/)

  * [ORBSLAM数据集、evo评估工具介绍](https://blog.csdn.net/weixin_42751489/article/details/115533441)——[Z-way](https://blog.csdn.net/weixin_42751489)

  * [第二篇 SLAM相关数据集：全面调研](https://zhuanlan.zhihu.com/p/471979416)——[不浪漫雷达](https://www.zhihu.com/people/Chaos)

  * [SLAM相关学习资料：综述/激光/视觉/数据集/常用库](https://zhuanlan.zhihu.com/p/434874344)——[菠萝包包包](https://www.zhihu.com/people/bo-luo-bao-ba-ba)

  * [深度学习公共数据集（五）：行人检测、遥感、机器人技术、图像分割、SLAM](https://zhuanlan.zhihu.com/p/359951922)——[石坚](https://www.zhihu.com/people/bu-you-ya-xian-sheng-46)

  * [超详细的计算机视觉数据集汇总（自动驾驶、SLAM、三维重建、立体视觉、深度估计）](https://zhuanlan.zhihu.com/p/99680662)——[Tom Hardy](https://www.zhihu.com/people/18301926762)

  * [汇总 | SLAM、重建、语义相关数据集大全](https://zhuanlan.zhihu.com/p/68294012)——[计算机视觉life](https://www.zhihu.com/people/cheng-xu-yuan-10)
  * [关于我的研究方向：GNSS 数据下载链接汇总](https://zhuanlan.zhihu.com/p/619226024)——[Lwcah](https://www.zhihu.com/people/lwcah-17)
  * [北斗/GNSS相关数据下载地址合集（未整理完）GAMIT/BERNESE](https://zhuanlan.zhihu.com/p/285099374)——[ThreeI](https://www.zhihu.com/people/Three_I)



---

* [PSINS](http://www.psins.org.cn/dhsj)：严恭敏老师维护的捷联惯导和组合导航数据，还有标定、对准数据，有几十组，各种数据，各种精度的 IMU都有，严老师提供了详细的说明，和对应的 PSINS 主函数，缺点是很多数据缺乏参考真值。包括：
  * [mti-300car4p8h跑车数据](http://www.psins.org.cn/newsinfo/6839783.html)：Mti-300跑车，采样频率100Hz，总采集时间约为4.8小时。包含MEMS陀螺仪、加速度计、三轴地磁，以及mti输出的姿态方位数据，可作为航姿算法对比验证使用。Allan方差分析陀螺精度约10°/h。
  * [某惯性级惯导在一段直路上来回跑车测试](http://www.psins.org.cn/newsinfo/6769471.html)：某惯性级光纤惯导在一段约3km直路上来回跑车测试四次，惯导采集频率200Hz，GPS采集1Hz。数据采集时长从0.7小时~7小时，在f024imugps.mat文件内共含四组变量（imu1,gps1；imu2,gps2；imu3,gps3；imu4,gps4），每组对应一次测试。
  * [某惯性级光纤惯导手动系统级标定](http://www.psins.org.cn/newsinfo/6403328.html)：某惯性级光纤惯导经过两三年储存，标定系数可能有些变化。在西工大长安校区惯性技术实验室进行测试试验，光纤惯导采集频率100Hz，先5min静止，再不断转动-静止，共采集约1600s，仅采用手动方法进行翻转，即可实现系统级标定，具体转动方案及标定原理参见“惯性仪器测试与数据分析，第9章”。
  * [基于惯性的管线检测原始数据](http://www.psins.org.cn/newsinfo/6079021.html)：采用惯导进行地下管线测量的原始MEMS-IMU和里程仪数据，管线长度约250m，共来回拉了两次，可为惯导/里程仪组合导航算法验证使用。可将起始点坐标当作0，去程末点相对坐标：东向 154.337m；北向 201.751m；天向-0.254m。
  * [一静态组合导航数据（无平滑效果）](http://www.psins.org.cn/newsinfo/4552096.html)：MEMS-IMU 100Hz（stim300），CDGNSS 1Hz（mm级定位），时常480s。
  * [无误差的惯导飞行数据](http://www.psins.org.cn/newsinfo/4445782.html)：一组无人机惯导飞行数据，飞行速度100m/s，距离约100km，采集时间4000s，频率200Hz，前200s静止。采用INS/DGNSS组合平滑后，再反演生成无误差的IMU和AVP参考值（原理参见文献“基于实测轨迹的高精度捷联惯导模拟器”），可为惯导算法验证使用。
  * [千分之二的激光惯导静态测试数据](http://www.psins.org.cn/newsinfo/4262859.html)：实验室中激光陀螺惯导静止采集数据4小时，采集频率100Hz。
  * [14小时的IMU和地磁测试数据](http://www.psins.org.cn/newsinfo/3003364.html)：MEMS-IMU（加3轴地磁）静止放置，采集频率10Hz，输出姿态/航向、三轴陀螺、三轴加计、三轴磁数据，总采集时间约为14小时（从晚6：40到第二天早9：00）。
  * [三组MTI-300航姿仪跑车数据](http://www.psins.org.cn/newsinfo/2802067.html)：MTi-300在北京地区的测试数据（含IMU、地磁和MTi的姿态航向输出），采样率100Hz，网友可用作练习对比自己的AHRS算法。
  * [四组光纤陀螺罗经摇摆对准数据](http://www.psins.org.cn/newsinfo/2770463.html)：光纤陀螺罗经系统在转台上做三轴摇摆试验（俯仰10°/5s、横滚20°/8.3s、方位5°/7s），先摇摆15min再静止约30s，对准结果分别为 45.02/134.93/89.93/179.65°。
  * [FOG-IMU/GPS紧组合数据](http://www.psins.org.cn/newsinfo/2678646.html)：FOG-IMU中光纤陀螺精度约 0.5°/h，采集频率200Hz；GNSS采集了GPS的导航电文和1Hz的伪距和多普勒观测量。在西北工业大学长安校区校园内跑车，总采集时间为0.5小时。
  * [超高精度MEMS-IMU静态8h数据](http://www.psins.org.cn/newsinfo/2449125.html)：在实验室静基座条件下，对某型号MEMS-IMU从开机起进行静态数据采集，采样频率100Hz，总采集时间为8小时。Allan方差精度达到0.1°/h。
  * [惯导-卫导-天文跑车数据](http://www.psins.org.cn/newsinfo/2408290.html)：惯性级激光惯导频率200Hz；GPS单点定位1Hz；CNS为大视场星敏，标称精度20角秒，星敏输出星敏相对于惯性系的姿态四元数 $q^i_s$，1Hz采样，动态测量范围小于2°/s。跑车地点在西工大长安校区，时长约2000s。由于惯导和星敏之间没有做较好的标定，此数据主要是为了展示星敏坐标转换（将测量从惯性坐标系转换至地理坐标系，工具箱新增了天文导航功能函数）。
  * [IMU-FSAS与载波相位差分GNSS组合数据](http://www.psins.org.cn/newsinfo/2347360.html)：跑车采集FSAS/GNSS数据，IMU采样频率200Hz，GNSS为1Hz，总采集时间为2000s，附有IE后处理的20Hz参考基准。
  * [一段铁轨检测的测试数据](http://www.psins.org.cn/newsinfo/2064505.html)：惯性级光纤惯导，输出频率200Hz；差分GNSS（cm级），输出频率1Hz。静态对准后，在铁轨上行往返走600多米。
  * [三组cpt-igm-kvh跑车测试](http://www.psins.org.cn/newsinfo/1590797.html)：cpt-igm-kvh三种类型惯导跑车测试分别为：（1）cpt中，IMU采样频率为100Hz；GPS采用差分，输出频率1Hz；IE处理的AVP-eb-db输出100Hz。（2）igmt中，IMU采样频率为200Hz；GPS采用差分，输出频率1Hz；IE处理的AVP-eb-db输出100Hz。（3）kvh中，IMU采样频率为200Hz；GPS采用差分，输出频率1Hz；IE处理的AVP-eb-db输出100Hz。
  * [光纤惯组SPANISA跑车测试](http://www.psins.org.cn/newsinfo/1560060.html)：FOG惯组为IMU-ISA-100C，采样频率为100Hz；GPS采用差分，输出频率10Hz。先静止690s（可自主对准），然后跑车约5460s，再静止300s。该数据中有IE软件处理的AVP参考输出10Hz，可做对比分析。
  * [光纤惯组KVH1775/GPS跑车测试](http://www.psins.org.cn/newsinfo/1512126.html)：FOG惯组为KVH1775，采样频率为100Hz；GPS采用差分，输出频率1Hz。先静止100s（可自主对准），再跑车约5300s。该数据中有某软件处理的AVP参考输出，可做对比分析，值得仔细分析！
  * [memsfoggps跑车测试](http://www.psins.org.cn/newsinfo/1508278.html)：MEMS惯组为MTI710工作在垂直陀螺模式下（含水平姿态输出），FOG惯组为惯性级别，两IMU采样频率均为100Hz；GPS采用差分，输出频率5Hz。先静止730s，再跑车约3小时，车速较慢约3.5m/s，行车范围约方圆10km内，外行再返回。该数据可用作FOG纯惯性、MEMS/GPS组合、FOG/GPS组合、MEMS/FOG传递对准、MEMS航姿对比等试验。
  * [MEMS-GNSS跑车数据C++处理](http://www.psins.org.cn/newsinfo/1508277.html)：MEMS-IMU采集频率100Hz，GNSS频率5Hz（RTK差分、少部分单点），跑车约600s。含Matlab和C++演示。
  * [MEMS-MAG-BARO-GNSS-SPP-RTK跑车数据](http://www.psins.org.cn/newsinfo/1500896.html)：MEMS-IMU/MAG/BARO采集频率200Hz，GNSS频率5Hz（有单点和RTK差分两种输出），先静止1300s，再跑车约400s。
  * [MPU6500-GNSS跑车数据](http://www.psins.org.cn/newsinfo/1500895.html)：MPU6500-MEMS-IMU采集频率100Hz，GNSS频率1Hz（只有单点定位的位置输出、没有速度），跑车时间长度约2300s，2080s后GNSS丢星约100s。
  * [激光惯导与GNSS跑车数据](http://www.psins.org.cn/newsinfo/1382666.html)：激光惯导中陀螺精度约0.01°/h，输出频率为200Hz，GNSS频率1Hz，数据长度约4300s，先静止对准600s，再行驶1小时。
  * [高精度MEMS-GNSS跑车数据](http://www.psins.org.cn/newsinfo/1187694.html)：高精度MEMS-IMU（ 陀螺零偏1 deg/h、加表零偏100ug）采集频率100Hz，差分GNSS频率5Hz，跑车时间长度约30min。
  * [高精度MEMS-FOG-GNSS跑车数据](http://www.psins.org.cn/newsinfo/1155264.html)：高精度MEMS-IMU（ 陀螺零偏0.3 deg/h、加表零偏50ug）采集频率200Hz，光纤FOG-IMU（ 陀螺零偏0.02 deg/h、加表零偏50ug）采集频率200Hz，差分GNSS频率5Hz，跑车时度约4000s。
  * [模拟战斗机摇翼传递对准的跑车试验数据](http://www.psins.org.cn/newsinfo/1087008.html)：Stim300-MEMS子惯导采集频率100Hz；惯性级光纤主惯导与GNSS作组合，AVP输出频率10Hz，数据总长度100s。载车大致沿直线从南往北行驶，速度约15m/s，中间主子惯导共同的安装支架绕车辆横滚方向做1.5个周期近似正弦摇摆（模拟战斗机摇翼）。安装支架基本刚性、主子惯导间并排、杆臂很小可忽略，已经过同步处理（不同步10ms量级）。可用作“姿态+速度”匹配传递对准算法验证使用。
  * [STIM300-GNSS跑车数据(有姿态参考)](http://www.psins.org.cn/newsinfo/958984.html)：STIM300采集频率125Hz，单点GNSS频率1Hz，在西工大长安校区内跑车，长度约50分钟，其中有惯性级光纤惯导给出的姿态结果作为参考基准。STIM/FOG/GNSS之间的时间不同步在ms量级。
  * [BMI055在移动机器人上的测试](http://www.psins.org.cn/newsinfo/885517.html)：BMI055-MEMS-IMU在移动机器人进行了两组测试，先是进行了两小时的静态测试；再进行了平地上半小时来回行走测试。
  * [MEMS-双天线GNSS-地磁-气压跑车数据](http://www.psins.org.cn/newsinfo/2370168.html)：MEMS采集频率100Hz，GNSS频率1Hz，在西安市跑车数据，长度约20分钟，可用作MEMS/双天线GNSS/地磁/气压组合导航或车载航姿算法验证使用。
  * [某MEMS-IMU几组振动测试](http://www.psins.org.cn/newsinfo/2321331.html)：MEMS-IMU安装在线振动台上，沿天向轴做随机或扫频振动。
  * [MTI几组超量程测试](http://www.psins.org.cn/newsinfo/2321280.html)：在实验室电脑旁进行手动大角速度摇晃或大比力冲击测试（最后还有一组跑车测试数据），数据采集400Hz，不接GNSS不接地磁。晃动过程中的姿态精度是无法考核的，而主要查看水平姿态角在超量程后的恢复能力。
  * [光纤惯导的系统级标定数据](http://www.psins.org.cn/newsinfo/2314054.html)：光纤惯导中陀螺精度约0.03°/h，惯导频率200Hz，数据长度约1h。
  * [两组激光惯导里程计与GNSS跑车数据](http://www.psins.org.cn/newsinfo/2313959.html)：激光惯导中陀螺精度优于0.01°/h，惯导和里程计输出频率均为125Hz，GNSS频率1Hz（只有位置输出，采用串口采集与IMU不太同步），每组数据长度约4200s，路程40km，先静止对准600s，可用于里程计辅助组合导航（里程计安装误差和刻度系数需自己细调）。
  * [五组激光惯导与GNSS跑车数据](http://www.psins.org.cn/newsinfo/2313820.html)：激光惯导中陀螺精度优于0.01°/h，惯导和里程计输出频率均为125Hz，GNSS频率1Hz（只有位置输出，采用串口采集与IMU不太同步），每组数据长度约20-30min，路程约3-4km，先静止再跑车再静止，可用于里程计辅助动基座对准测试（里程计安装误差和刻度系数需自己细调）。
  * [光纤惯导与GNSS跑车数据](http://www.psins.org.cn/newsinfo/2310099.html)：光纤惯导中陀螺精度约0.03°/h，惯导频率200Hz，GNSS频率1Hz，从西工大新校区开始跑到市内，数据长度约4300s。
  * [STIM300与载波相位差分GNSS组合数据](http://www.psins.org.cn/newsinfo/2295380.html)：载车上采集MEMS-STIM300/GNSS数据，采样频率200Hz，总采集时间为2200s。
  * [伪造的超高精度惯导数据（一小时误差1m）](http://www.psins.org.cn/newsinfo/2294622.html)：这是我们'制造'的一套世界上最高精度的捷联惯导跑车实测数据（通过跑车实测组合导航轨迹数据反算IMU陀螺和加速度计信息，即惯性器件反演算法），采样周期10ms，数据长度约1小时，纯惯导精度约为1m/h。
  * [惯性级惯导载车上静止30min数据](http://www.psins.org.cn/newsinfo/2294590.html)：在载车上，对某惯性级别激光陀螺捷联惯导进行静态数据采集，有人员上下车晃动干扰，总采集时间为半小时。
  * [STIM300静止3h数据](http://www.psins.org.cn/newsinfo/2294190.html)：在实验室静基座条件下，对某MEMS-IMU进行静态数据采集，采样频率100Hz，总采集时间为3小时。
  * [SBG-ELLIPSE2-A高铁搭载试验](http://www.psins.org.cn/newsinfo/2294175.html)：SBG-ELLIPSE2-A放置在高铁车厢地板上。
  * [MTI高铁测试](http://www.psins.org.cn/newsinfo/2294171.html)：MTI-710设置为航姿工作模式（不外接卫导），放置在高铁车厢窗户边上。

---

* [武大i2Nav开源](http://i2nav.cn/index/newListDetail_zw.do?newskind_id=13a8654e060c40c69e5f3d4c13069078&newsinfo_id=9fe50feadb3a499ea5c5e5bbd32ff9e1)：武大i2Nav这几套数据质量很高，有很详细的说明、配套的开源程序、论文、参考真值。
  * [i2NAV-awesome-gins-datasets](https://github.com/i2Nav-WHU/awesome-gins-datasets)：本数据集采集于湖北省武汉市一处工业园区，为开阔天空场景，GNSS  RTK定位良好。数据集包括GNSS定位结果、IMU原始数据和高精度参考真值，以及对应的噪声参数和安装参数。本数据集最大的特点是提供了四种不同型号的MEMS  IMU数据，包括消费级MEMS芯片和工业级MEMS模块。相关论文见：https://arxiv.org/abs/2109.03010。
  * [Wheel-INS](https://github.com/i2Nav-WHU/Wheel-INS)： 本数据集采集于武汉大学校园，实验载体分别为轮式机器人和汽车，参考系统为高精度GNSS/INS组合导航系统。数据集包括车轮安装MEMS IMU原始数据，车身安装MEMS IMU原始数据，里程计数据（速度）和位姿参考真值。本数据集同时开源了相关的基于车轮安装IMU的航迹推算系统代码。相关论文见：https://doi.org/10.1109/TVT.2021.3102409、https://doi.org/10.1109/TVT.2021.3108008。
  * [IMU-Array](https://github.com/i2Nav-WHU/IMU-Array)：本数据集包括IMU阵列(由16个消费级IMU ICM20602组成)的动态测试数据和标定数据。其中动态测试数据采集于湖北省武汉市一处开阔天空场景，数据包括16个消费级IMU的原始测量值、16个IMU进行标定补偿后的测量值，同时提供GNSS RTK定位结果和高精度惯导计算得到的参考真值。标定数据采集于武汉大学测绘学院三轴转台实验室，按照加速度计六位置法和陀螺仪角位置法采集标定数据，并提供标定过程中加速度和角速度真值，以及加速度计和陀螺仪的标定算法。
  * [GIOW-release](https://github.com/i2Nav-WHU/GIOW-release)：开源了针对全向轮式机器人的GNSS/INS/ODO/WheelAngle 算法代码以及相关的数据。本数据集采集于武汉大学信息学部操场，实验载体为全向轮式机器人，参考系统为高精度GNSS/INS组合导航系统。数据集包括GNSS数据、MEMS IMU原始数据、里程计数据和轮角数据。使用方法详见readme.md，相关论文见：https://doi.org/10.1088/1361-6501/ac17fb。

---

* [武大USI开源](https://github.com/WHU-USI3DV)：武大国重杨必胜的城市空间智能（USI）研究小组，
  * [WHU-TLS](https://github.com/WHU-USI3DV/WHU-TLS)：大型三维激光扫描仪数据，由115个扫描点组成，总计超过1.74亿个三维点，采集自11种不同环境（即地铁站、高铁站台、山地、森林、公园、校园、住宅、河岸、文物建筑、地下挖掘和隧道），点密度、杂波和遮挡情况各不相同。还为研究人员提供了地面实况变换、Dong 等人（2018 年）计算的变换以及配准图，目的是在一个共同的基础上更好地比较和了解不同配准方法的优缺点。我们希望该基准能满足研究界的需求，并成为开发前沿 TLS 点云注册方法的重要数据集。此外，建议的基准还能为铁路安全运行、河流调查与治理、森林结构评估、文化遗产保护、滑坡监测和地下资产管理等应用提供合适的数据集。数据申请网址：https://wj.qq.com/s2/12803941/a30e，相关论文：https://doi.org/10.1016/j.isprsjprs.2020.03.013。
  * [WHU-Helmet](https://github.com/kafeiyin00/WHU-HelmetDataset)： 头盔式多源融合SLAM数据集（GNSS/IMU/Camera/Lidar），采集包括采集自11组不同环境的数据（即地铁站、高铁站台、山地、森林、公园、校园、住宅、河岸、文物建筑、地下挖掘和隧道）。提供了  LiDAR-IMU，LiDAR-Camera 的外参和 Camera 的内参。数据以 Rosbag 形式给出，可以运行在 [fast-lio2](https://github.com/hku-mars/FAST_LIO) 下运行的配置文件， 在 [Fast_lio](https://github.com/hku-mars/FAST_LIO)、[LIO_Livox](https://github.com/Livox-SDK/LIO-Livox)、[LOAM_Livox](https://github.com/hku-mars/loam_livox) 和 [MULLS](https://github.com/YuePanEdward/MULLS) 都进行了测试并给出了测试结果。相关论文：https://doi.org/10.1109/TGRS.2023.3275307。
  * [WHU-Urban-3D](https://whu3d.com/)：数据集广泛覆盖了机载和移动激光扫描点云，为三维深度学习算法的发展提供了一个独特而富有挑战性的平台。该数据集是在中国熙熙攘攘的大都市中精心采集的，城市场景的复杂性和可变性极高。通过WHU-Urban3D，我们旨在弥补高质量数据集的有限可用性与现实世界室外环境中对三维场景理解日益增长的需求之间的差距。我们希望通过提供注释丰富、内容广泛的数据集，鼓励研究人员探索新的前沿领域，推动三维深度学习应用的发展，从而为令人兴奋的研究想法和创新发展提供更多机会。相关论文：https://doi.org/10.1016/j.isprsjprs.2024.02.007。
  * [WHU-Railway3D](https://github.com/WHU-USI3DV/WHU-Railway3D)：点云语义分割（PCSS）在为数字孪生铁路生成精确的三维语义地图方面显示出巨大的潜力。在众多 PCSS 数据集的推动下，基于深度学习的方法取得了长足进步。然而，现有数据集往往忽视铁路场景，在规模、类别和场景多样性方面存在局限性。这促使我们建立了 WHU-Railway3D，一个专为铁路场景设计的多样化PCSS数据集。WHU-Railway3D 根据场景复杂程度和语义类别分布将铁路分为城市铁路、农村铁路和高原铁路。该数据集跨度为30公里，包含38亿个点，标注为11个类别（如铁轨、架空线）。除三维坐标外，WHU-Railway3D 还提供了丰富的属性信息，如反射强度和回波数。在数据集上对先进方法进行了广泛评估，并随后进行了深入分析。最后，确定了关键挑战和潜在的未来工作，以激励进一步的创新研究。数据申请链接：https://wj.qq.com/s2/13387420/ac80，
    * 城市铁路数据集是使用 Optech 的 Lynx 移动测绘系统在位于中国中部的一个城市路段采集的，覆盖道路长度约 10.7 公里。与街道道路基准相比，城市铁路数据集是在复杂的铁路环境中采集的，其中包含更多噪声和具有挑战性的铁路相关类别。
    * 农村铁路数据集是使用配备两个 HiScan-Z 激光雷达传感器的 MLS 系统在野外采集的，覆盖道路长度约 10.6 公里。该数据集面临各种挑战，包括植被和地形变化造成的遮挡。
    * 高原铁路数据集是使用配备 32 线激光雷达传感器的铁路移动测量系统 (rMMS) 在高原地区获得的，道路长度约为 10.4 公里。高原铁路数据集由于不同类别之间的点数分布不平衡而构成挑战，因此需要制定策略来解决针对不同模式的均衡学习问题。

---

* [港理工IPNL开源](https://github.com/IPNL-POLYU)：他们还开源了图优化 GNSS 程序  [GraphGNSSLib](https://github.com/weisongwen/GraphGNSSLib) 和图优化紧组合GNSS/LiDAR/IMU 程序 [GLIO](https://github.com/XikunLiu-huskit/GLIO)。
  * [UrbanNavDataset](https://github.com/weisongwen/UrbanNavDataset)：在包括东京和香港在内的亚洲城市峡谷中收集的本地化数据开放源代码。在城市峡谷深处使用低成本传感器进行定位仍然是一个具有挑战性的问题。在城市峡谷中，由于高楼林立，导致大量非视距（NLOS）接收和多径效应，GNSS 的精度受到严重挑战。此外，过多的动态物体也会扭曲激光雷达和相机的性能。UrbanNav 数据集希望为社区提供一个具有挑战性的数据源，以进一步加快在具有挑战性的城市峡谷中进行精确、稳健定位的研究。该数据集包括来自 GNSS 接收机、激光雷达、相机和 IMU 的传感器测量数据，以及来自 SPAN-CPT 系统的精确地面实况。与 Waymo、KITTI 等现有数据集不同，UrbanNav 提供原始 GNSS RINEX 数据。在这种情况下，用户可以通过原始数据提高 GNSS 定位的性能。总之，UrbanNav 数据集特别注重改善城市峡谷中的 GNSS 定位，同时还提供来自激光雷达、摄像头和 IMU 的传感器测量数据。
  * [UrbanLoco](https://github.com/weisongwen/UrbanLoco)： 用于城市场景制图和定位的全传感器套件数据集。测绘和定位是自动驾驶的关键模块，在这一领域已取得了重大成就。除全球导航卫星系统（GNSS）外，点云注册、视觉特征匹配和惯性导航方面的研究也大大提高了不同场景下绘图和定位的准确性和鲁棒性。然而，高度城市化的场景仍然具有挑战性： 基于激光雷达和摄像头的方法在面对大量动态物体时表现不佳；基于全球导航卫星系统的解决方案存在信号丢失和多径问题；惯性测量单元（IMU）存在漂移问题。遗憾的是，目前的公共数据集要么没有充分应对这一城市挑战，要么没有提供足够的与测绘和定位相关的传感器信息。在此，我们介绍 UrbanLoco：这是一个在高度城市化环境中收集的制图/定位数据集，具有完整的传感器套件。该数据集包括在旧金山和香港采集的 13 条轨迹，总长度超过 40 公里。我们的数据集包括各种城市地形：城市峡谷、桥梁、隧道、急转弯等。更重要的是，我们的数据集包括来自激光雷达、摄像头、IMU 和 GNSS 接收机的信息。
    * 

---

* 港科技开源：

  * [GVINS-Dataset](https://github.com/HKUST-Aerial-Robotics/GVINS-Dataset)：开源程序 GVINS 配套数据集，
  * [MASSTAR](https://github.com/HKUST-Aerial-Robotics/MASSTAR)：表面预测和补全已经在各种应用中广泛研究，用于多样化的下游任务。随着相关研究的发展，表面补全算法从对象级别发展到场景级别，从虚拟场景扩展到真实世界，从单模态发展到多模态。当前的数据集表现出不尽如人意的性能和挑战不足，这严重制约了该领域的发展。在本文中，我们提出了MASSTAR：面向表面预测和补全任务的多模态大规模场景数据集和多功能工具链。我们收集了大量场景级别模型，包括来自各种开源作品的部分真实世界捕获数据。我们还开发了一个工具链，以便通过对原始3D数据进行分割、选择有价值的模型并生成包括RGB图像、描述性文本、深度图像和部分点云在内的多模态数据来处理数据。此外，我们对在我们的数据集上训练的不同算法进行了基准测试。我们将MASSTAR与当前数据集进行了比较，验证了我们系统的优越性。

---

* [OpenDriveLab](https://github.com/OpenDriveLab)：
  * [DriveLM](https://github.com/OpenDriveLab/DriveLM)：
  * [LightwheelOcc](https://github.com/OpenDriveLab/LightwheelOcc)：
  * [OpenLane-V2](https://github.com/OpenDriveLab/OpenLane-V2)：

---

* [中山大学CPNT开源](https://github.com/SYSU-CPNTLab)：
  * [SYSU-Campus-GVI-Dataset](https://github.com/SYSU-CPNTLab/SYSU-Campus-GVI-Dataset)：我们收集了中山大学校园内六个代表性场景的数据。其中，"宿舍楼 "和 "工程楼 "场景具有城市峡谷的特点，受周围高楼或山林的影响，整个场景都是高楼林立。在体育馆大楼和医疗大楼场景中，数据采集设备从开阔的室外进入室内，然后又回到室外。而 SYSU Campus 和 Athletic Field 场景则分别在大规模和重复环境中采集。SYSU-Campus-GVI 数据集的总持续时间约为 3611 s，总行驶距离约为 16456 m。相关论文：https://doi.org/10.1109/JIOT.2024.3379755。
  * [GVI-SYSU-Outdoor-Indoor-Dataset](https://github.com/SYSU-CPNTLab/GVI-SYSU-Outdoor-Indoor-Dataset)：该数据集包含在地下车库和建筑物中采集的复杂的室内-室外转换 GNSS 视觉-惯性数据集。我们的多传感器数据采集设备用途广泛，既可用于车载应用，也可用于手持应用。该设备集成了多种传感器，包括 Realsense D455 的立体相机和 IMU、VRTK2 的两个用于 RTK 定位信息的 U-blox ZED-F9P 模块，以及用于压力传感器（气压计）测量的智能手机。所有传感器数据均使用时间戳进行同步。我们在各种复杂的室内外场景中收集了五个数据集，包括室外环境、地下位置和建筑物内部。相关论文：https://doi.org/10.1109/TCSII.2024.3351172。
  * [LBL-AQUALOC-Dataset](https://github.com/SYSU-CPNTLab/LBL-AQUALOC-Dataset)：公共水下数据集 AQUALOC 收集了水下考古遗址场景中的十个序列数据。为了解决 LBL 数据缺乏的问题，我们利用半物理模拟构建了一个新的数据集 LBL-AQUALOC。这一新数据集包括来自 LBL、单相机、PS 和低成本 MEMS-IMU 的测量数据，以 Rosbag 格式打包。相关论文：https://doi.org/10.1109/LRA.2023.3334979。

---

* 上海交大ViSYS开源：

  * [M2DGR](https://github.com/SJTU-ViSYS/M2DGR)： 适用于地面机器人的多模式、多场景 SLAM 数据集。这是一个由地面机器人收集的新型大规模数据集，该机器人配有全套传感器套件，包括六个鱼眼和一个天空指向型 RGB 摄像机、一个红外摄像机、一个事件摄像机、一个视觉惯性传感器（VI-sensor）、一个惯性测量单元（IMU）、一个激光雷达、一个消费级全球导航卫星系统（GNSS）接收器和一个带实时运动学（RTK）信号的 GNSS-IMU 导航系统。所有这些传感器都经过良好校准和同步，并同时记录数据。地面真实轨迹由动作捕捉装置、激光 3D 跟踪器和 RTK 接收器获得。数据集由 36 个序列（约 1 TB）组成，在包括室内和室外环境在内的不同场景中捕获。我们在 M2DGR 上评估了最先进的 SLAM 算法。结果表明，现有解决方案在某些场景下表现不佳。为了研究界的利益，我们公开了数据集和工具。相关论文：https://doi.org/10.1109/LRA.2021.3138527。

  * [M2DGR-plus](https://github.com/SJTU-ViSYS/M2DGR-plus)：扩展和更新 M2DGR：用于地面机器人的新型多模式、多场景 SLAM 数据集。配套开源程序：[Ground-Fusion](https://github.com/SJTU-ViSYS/Ground-Fusion)，相关论文：https://doi.org/10.48550/arXiv.2402.14308。
  * [TextSLAM-Dataset](https://github.com/SJTU-ViSYS/TextSLAM-Dataset)：
  * [SJTU_GVI](https://github.com/sjtuyinjie/SJTU_GVI)：
  * [Ground-Challenge](https://github.com/sjtuyinjie/Ground-Challenge)：
  * [EKF_GI_IM_Dataset](https://github.com/sjtuyinjie/EKF_GI_IM_Dataset)：
  * [NTU VIRAL](https://ntu-aris.github.io/ntu_viral_dataset/)：

---

* [芬兰TIERS](https://github.com/TIERS)：

  * [multi_lidar_multi_uav_dataset](https://github.com/TIERS/multi_lidar_multi_uav_dataset)：
  * [uwb-relative-localization-dataset](https://github.com/TIERS/uwb-relative-localization-dataset)：
  * [tiers-lidars-dataset](https://github.com/TIERS/tiers-lidars-dataset)：
  * [tiers-lidars-dataset-enhanced](https://github.com/TIERS/tiers-lidars-dataset-enhanced)：
  * [uwb-drone-dataset](https://github.com/TIERS/uwb-drone-dataset)：

---

* [卡内基梅隆大学机器人研究所AirLab开源](https://github.com/castacks)：

  * [TartanAviation](https://github.com/castacks/TartanAviation)：
  * [alfa-dataset-tools](https://github.com/castacks/alfa-dataset-tools)：
  * [WIT-UAS-Dataset](https://github.com/castacks/WIT-UAS-Dataset)：
  * [tartanair_tools](https://github.com/castacks/tartanair_tools)：
  * [tartan_drive](https://github.com/castacks/tartan_drive)：
  * [tartanairpy](https://github.com/castacks/tartanairpy)：
  * [alfa-dataset](https://github.com/castacks/alfa-dataset)：
  * [mbzirc_dataset](https://github.com/castacks/mbzirc_dataset)：
* [ntnu-arl](https://github.com/ntnu-arl)：
  * [underwater-datasets](https://github.com/ntnu-arl/underwater-datasets)：
  * [ballast_water_tank_dataset](https://github.com/ntnu-arl/ballast_water_tank_dataset)：
  * [lidar_degeneracy_datasets](https://github.com/ntnu-arl/lidar_degeneracy_datasets)：

---

* 3DOM-FBK开源：
  * [NeRFBK](https://github.com/3DOM-FBK/NeRFBK)：
  * [Collaborative_Navigation](https://github.com/3DOM-FBK/Collaborative_Navigation)：
  * [RF4PCC](https://github.com/3DOM-FBK/RF4PCC)：
  * [MIN3D](https://github.com/3DOM-FBK/MIN3D)：

---

* 知名SLAM数据集：

  * [KITTI数据集](http://www.cvlibs.net/datasets/kitti/eval_object.php)：
  * [EuRoC数据集](https://projects.asl.ethz.ch/datasets/doku.php?id=kmavvisualinertialdatasets)：
  * [TUM数据集](https://vision.in.tum.de/data/datasets/rgbd-dataset/download)：
  * [Oxford数据集](https://robotcar-dataset.robots.ox.ac.uk/)：
  * [ICL-NUIM数据集](http://www.doc.ic.ac.uk/~ahanda/VaFRIC/iclnuim.html)：
  * [RGB-D对象数据集](http://rgbd-dataset.cs.washington.edu/)：
  * [Dynamic-Scenes 数据集](https://github.com/HaoshengChen/Dynamic-Scenes)：
  * [awesome-slam-datasets 整理](https://github.com/youngguncho/awesome-slam-datasets)：

---

* IMU人体姿态数据集：
  * 



---

* **UWB相关数据集**：

  > * **时间同步**：UWB有四种测矩值，信号到达时间（TOA）、信号到达时间差（TDOA）、信号往返时间（TOF）、信号到达角（AOA），其中 TOA 需要基站和标签都进行时间同步，TODA 需要基站时间同步，AOA 和 TOF 无需时间同步。
  > * **基准站坐标**：注意用什么坐标系

  * [uwb-drone-dataset](https://github.com/TIERS/uwb-drone-dataset)：基于 UWB 的无人机定位系统在 GNSS 信号缺失的环境中的应用： 特征和数据集。该资源库包含一个数据集，用于研究 UWB 辅助无人飞行器自主飞行的准确性，还包含 ROS 节点，用于将 Decawave 的 DWM1001 节点设置为主动标签或被动标签时与之连接。最后，我们还提供了用于重新编程 DWM1001 DEV 板的固件，以获得快速准确的锚点自动定位。数据集采用两种不同的方法记录，即在无人飞行器上或在地面站记录rosbags。第一种情况包括使用配备有源 UWB 标签并自主飞行的四旋翼飞行器上的机载计算机获取的数据。第二种情况是指四旋翼飞行器手动飞行时，与地面站连接的无源标签所获取的数据。在第二种情况下，由于用于记录位置的标签是被动的，因此运动捕捉系统提供的地面实况数据与 UWB 数据之间存在延迟。主飞行测试由 F450 四旋翼飞行器完成，该飞行器配备了运行 PX4 固件的 Pixhawk 2.4、作为配套计算机运行 Ubuntu 16.04 的英特尔 Up Board、ROS Kinetic 和 MAVROS，以及用于高度估计的 TF Mini 激光雷达。相关论文：https://doi.org/10.1109/IROS45743.2020.9341042
  * [UWB_TDOA_dataset](https://github.com/Williamwenda/UWB_TDOA_dataset)：我们提供了一个 UWB 到达时间差（TDOA）测量数据集。我们使用 TDOA2 模式基于 Bitcraze 的 Loco Position System（LPS）收集了 UWB 测量数据。地面实况由带有 10 个 Vicon 摄像头的运动捕捉系统提供。TDOA 测量由 Crazyflie 2.0 纳米四旋翼飞行器记录。来自 Vicon 系统的地面实况测量值与 UWB TDOA 测量值同步，以便计算 TDOA 测量偏差。利用从七个不同的 UWB 锚点星座收集的测量数据创建了一个训练数据集。神经网络用于捕捉和纠正 TDOA 测量偏差，以实现准确的状态估计。
  * [Industrial-UWB-localization-CIR-dataset](https://github.com/JaronFontaine/Industrial-UWB-localization-CIR-dataset)：包含信道脉冲响应数据（CIR）的工业 UWB 定位数据集。该数据集包含来自工业环境中 23 个不同位置的 CIR。这些信号来源于为估计距离而配置的双向测距标签-锚点对。如下图所示，该区域总共放置了 21 个锚点，允许视距（LOS）和非视距（NLOS）信号传播。参考论文：https://doi.org/10.1109/ACCESS.2020.3012822、https://doi.org/10.3390/s19071548。
  * [IR-UWB-Radar-Signal-Dataset-for-Dense-People-Counting](https://github.com/yangxiuzhu777/IR-UWB-Radar-Signal-Dataset-for-Dense-People-Counting)：用于人员计数的UWB数据集。该数据集由 376,000 个脉冲无线电超宽带（IR-UWB）雷达信号组成，这些信号来自三个密集场景中的最多 20 人，包括在每平方米 3 人和 4 人的受限区域中随机行走的 0-20 人，以及在平均距离为 10 厘米的队列中最多 15 人。
  * [UWB_Dataset](https://github.com/unmannedlab/UWB_Dataset)：
  * [uwb-nlos-human-detection](https://github.com/disi-unibo-nlp/uwb-nlos-human-detection)：

---

* [ewine-project](https://github.com/ewine-project)：
  * [SubGHz-technologies-dataset-Sigfox-LoRA-and-IEEE802.15.4g-subGHz-](https://github.com/ewine-project/SubGHz-technologies-dataset-Sigfox-LoRA-and-IEEE802.15.4g-subGHz-)：
  * [Enabling-Agile-Adaptation-in-Dense-Heterogeneous-Deployments-of-Commercial-802.11-Devices](https://github.com/ewine-project/Enabling-Agile-Adaptation-in-Dense-Heterogeneous-Deployments-of-Commercial-802.11-Devices)：
  * [sigfox-packet-datasets](https://github.com/ewine-project/sigfox-packet-datasets)：
  * [Sigfox-recorded-datasets](https://github.com/ewine-project/Sigfox-recorded-datasets)：
  * [Technology-classification-dataset](https://github.com/ewine-project/Technology-classification-dataset)：
  * [802_15_4_MAC_perf_datasets](https://github.com/ewine-project/802_15_4_MAC_perf_datasets)：

---

其它各种各样的数据集



